# -*- coding: utf-8 -*-
"""ClusteringSketch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19gjqCwTZMnFimuFwI6wZOrjZvhHUMFf8
"""

!pip install -q pyspark

# Commented out IPython magic to ensure Python compatibility.
from pyspark import SparkContext                                     #Importing SparkContext
from pyspark.sql import SparkSession, Window, Row                   # Importing importing methods for creating a cluster
from pyspark.sql import functions as F                              # Importing SQL Functions
from pyspark.sql.functions import col, isnan, when, count           # Importing relevant dataframe functions
from pyspark.sql.functions import *                                 # Importing inbuilt SQL Functions
from pyspark.sql.types import *                                     # Importing SQL types

import matplotlib.pyplot as plt                                     # Popular plotting library
# %matplotlib inline                                                  
import seaborn as sns                                               # Advanced plotting library
from handyspark import *                                            # Helper library to plot graphs

from pyspark.ml.feature import VectorAssembler                      # For processing dataset for ML
from pyspark.ml.regression import LinearRegression                  # Importing mlib linear regression
import warnings                                                     # Importing warning to disable runtime warnings
warnings.filterwarnings("ignore")

# Building a spark app/session
spark = SparkSession.builder.appName("Power_consumption").getOrCreate()

# single cluster information
spark

df = spark.read.options(header='True', inferSchema='True', delimiter=',') \
  .csv("/content/powerconsumption.csv")

type(df)

df.printSchema()

"""Data Post-Processing**
---

- We wll use a **VectorAssembler**.

- VectorAssember from **Spark ML library** is a module that allows to convert **numerical features into a single vector** that is used by the machine learning models.
"""

featureassembler = VectorAssembler(inputCols= ['Temperature','Humidity','WindSpeed','GeneralDiffuseFlows','DiffuseFlows'], outputCol = 'features')

output = featureassembler.transform(df)
output.show()

"""### **Feature Extraction**:"""

final_data = output.select("features", "PowerConsumption_Zone1")
final_data.show()

"""# **Data Split**"""

train_data, test_data = final_data.randomSplit(weights=[0.75,0.25], seed=42)

"""# **# Trains a k-means model.**"""

from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(train_data)

"""# **Make predictions**"""

predictions = model.transform(train_data)

"""# **Evaluate clustering by computing Silhouette score**"""

evaluator = ClusteringEvaluator()

silhouette = evaluator.evaluate(predictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

"""# **Shows the result.**"""

centers = model.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)